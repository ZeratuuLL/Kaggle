{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lifengwei/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from multiprocessing import Pool \n",
    "import tqdm \n",
    "import json\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "import sys\n",
    "from keras.utils import get_file\n",
    "\n",
    "import bz2\n",
    "import subprocess\n",
    "import xml.sax\n",
    "import mwparserfromhell \n",
    "import re\n",
    "import gc\n",
    "import json\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "root_path = '/Users/lifengwei/.keras/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../',\n",
       " '20190701/',\n",
       " '20190720/',\n",
       " '20190801/',\n",
       " '20190820/',\n",
       " '20190901/',\n",
       " '20190920/',\n",
       " '20191001/',\n",
       " 'latest/']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'https://dumps.wikimedia.org/enwiki/'\n",
    "index = requests.get(base_url).text\n",
    "soup_index = BeautifulSoup(index, 'html.parser')\n",
    "\n",
    "dumps = [a['href'] for a in soup_index.find_all('a') if \n",
    "         a.has_attr('href')]\n",
    "dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_url = base_url + dumps[-2]\n",
    "dump_html = requests.get(dump_url).text\n",
    "soup_dump = BeautifulSoup(dump_html, 'html.parser')\n",
    "link_list = soup_dump.find_all('li', {'class': 'file'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('enwiki-20191001-pages-articles-multistream.xml.bz2', ['16.3', 'GB']),\n",
       " ('enwiki-20191001-pages-articles-multistream-index.txt.bz2', ['205.7', 'MB']),\n",
       " ('enwiki-20191001-pages-articles-multistream1.xml-p10p30302.bz2',\n",
       "  ['173.0', 'MB']),\n",
       " ('enwiki-20191001-pages-articles-multistream-index1.txt-p10p30302.bz2',\n",
       "  ['163', 'KB']),\n",
       " ('enwiki-20191001-pages-articles-multistream2.xml-p30304p88444.bz2',\n",
       "  ['205.8', 'MB'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = []\n",
    "\n",
    "# Search through all files\n",
    "for file in link_list:\n",
    "    text = file.text\n",
    "    if 'pages-articles' in text:\n",
    "        files.append((text.split()[0], text.split()[1:]))\n",
    "        \n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enwiki-20191001-pages-articles27.xml-p54663464p56163464.bz2',\n",
       " 'enwiki-20191001-pages-articles27.xml-p56163464p57663464.bz2',\n",
       " 'enwiki-20191001-pages-articles27.xml-p57663464p59163464.bz2',\n",
       " 'enwiki-20191001-pages-articles27.xml-p59163464p60663464.bz2',\n",
       " 'enwiki-20191001-pages-articles27.xml-p60663464p61937279.bz2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_to_download = [file[0] for file in files if '.xml-p' in file[0]]\n",
    "files_to_download[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:00<00:00, 47379.18it/s]\n"
     ]
    }
   ],
   "source": [
    "data_paths = []\n",
    "file_info = []\n",
    "\n",
    "L = len(files_to_download)\n",
    "\n",
    "# Iterate through each file\n",
    "for file in tqdm.tqdm(files_to_download):\n",
    "    path = root_path + file\n",
    "    \n",
    "    # Check to see if the path exists (if the file is already downloaded)\n",
    "    if not os.path.exists(root_path + file):\n",
    "        # If not, download the file\n",
    "        data_paths.append(get_file(file, dump_url + file))\n",
    "        # Find the file size in MB\n",
    "        file_size = os.stat(path).st_size / (1024*1024)\n",
    "        \n",
    "        file_articles = int(file.split('p')[-1].split('.')[-2]) - int(file.split('p')[-2])\n",
    "        file_info.append((file, file_size, file_articles))\n",
    "        \n",
    "    else:\n",
    "        # If the file is already downloaded find some information\n",
    "        data_paths.append(path)\n",
    "        # Find the file size in MB\n",
    "        file_size = os.stat(path).st_size / (1024*1024)\n",
    "        \n",
    "        file_number = int(file.split('p')[-1].split('.')[-2]) - int(file.split('p')[-2])\n",
    "        file_info.append((file.split('-')[-1], file_size, file_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(title, text, timestamp, template = 'Infobox book'):\n",
    "    \"\"\"Process a wikipedia article looking for template\"\"\"\n",
    "    \n",
    "    # Create a parsing object\n",
    "    wikicode = mwparserfromhell.parse(text)\n",
    "    \n",
    "    # Search through templates for the template\n",
    "    matches = wikicode.filter_templates(matches = template)\n",
    "    \n",
    "    # Filter out errant matches\n",
    "    matches = [x for x in matches if x.name.strip_code().strip().lower() == template.lower()]\n",
    "    \n",
    "    if len(matches) >= 1:\n",
    "        # template_name = matches[0].name.strip_code().strip()\n",
    "\n",
    "        # Extract information from infobox\n",
    "        properties = {param.name.strip_code().strip(): param.value.strip_code().strip() \n",
    "                      for param in matches[0].params\n",
    "                      if param.value.strip_code().strip()}\n",
    "\n",
    "        # Extract internal wikilinks\n",
    "        wikilinks = [x.title.strip_code().strip() for x in wikicode.filter_wikilinks()]\n",
    "\n",
    "        # Extract external links\n",
    "        exlinks = [x.url.strip_code().strip() for x in wikicode.filter_external_links()]\n",
    "\n",
    "        # Find approximate length of article\n",
    "        text_length = len(wikicode.strip_code().strip())\n",
    "\n",
    "        return (title, properties, wikilinks, exlinks, timestamp, text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
    "    \"\"\"Parse through XML data using SAX\"\"\"\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._current_tag = None\n",
    "        self._books = []\n",
    "        self._article_count = 0\n",
    "        self._non_matches = []\n",
    "\n",
    "    def characters(self, content):\n",
    "        \"\"\"Characters between opening and closing tags\"\"\"\n",
    "        if self._current_tag:\n",
    "            self._buffer.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        \"\"\"Opening tag of element\"\"\"\n",
    "        if name in ('title', 'text', 'timestamp'):\n",
    "            self._current_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        \"\"\"Closing tag of element\"\"\"\n",
    "        if name == self._current_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)\n",
    "\n",
    "        if name == 'page':\n",
    "            self._article_count += 1\n",
    "            # Search through the page to see if the page is a book\n",
    "            book = process_article(**self._values, template = 'Infobox book')\n",
    "            # Append to the list of books\n",
    "            if book:\n",
    "                self._books.append(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_books(data_path, limit = None, save = True):\n",
    "    \"\"\"Find all the book articles from a compressed wikipedia XML dump.\n",
    "       `limit` is an optional argument to only return a set number of books.\n",
    "        If save, books are saved to partition directory based on file name\"\"\"\n",
    "\n",
    "    # Object for handling xml\n",
    "    handler = WikiXmlHandler()\n",
    "\n",
    "    # Parsing object\n",
    "    parser = xml.sax.make_parser()\n",
    "    parser.setContentHandler(handler)\n",
    "\n",
    "    # Iterate through compressed file\n",
    "    for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                             stdin = open(data_path), \n",
    "                             stdout = subprocess.PIPE).stdout):\n",
    "        try:\n",
    "            parser.feed(line)\n",
    "        except StopIteration:\n",
    "            break\n",
    "            \n",
    "        # Optional limit\n",
    "        if limit is not None and len(handler._books) >= limit:\n",
    "            return handler._books\n",
    "    \n",
    "    if save:\n",
    "        partition_dir = './data/'\n",
    "        # Create file name based on partition name\n",
    "        p_str = data_path.split('-')[-1].split('.')[-2]\n",
    "        out_dir = partition_dir + f'{p_str}.ndjson'\n",
    "\n",
    "        # Open the file\n",
    "        with open(out_dir, 'w') as fout:\n",
    "            # Write as json\n",
    "            for book in handler._books:\n",
    "                fout.write(json.dumps(book) + '\\n')\n",
    "        \n",
    "        print(f'{len(os.listdir(partition_dir))} files processed.', end = '\\r')\n",
    "\n",
    "    # Memory management\n",
    "    del handler\n",
    "    del parser\n",
    "    gc.collect()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114,\n",
       " '/Users/lifengwei/.keras/datasets/enwiki-20191001-pages-articles-multistream8.xml-p1268693p1791079.bz2')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitions = [root_path + file for file in os.listdir(root_path) if 'xml-p' in file]\n",
    "len(partitions), partitions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27764.362621568995 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "pool = Pool(processes = os.cpu_count()-1)\n",
    "\n",
    "start = timer()\n",
    "\n",
    "# Map (service, tasks), applies function to each partition\n",
    "results = pool.map(find_books, partitions)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "end = timer()\n",
    "print(f'{end - start} seconds elapsed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    \"\"\"Read in json data from `file_path`\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    # Open the file and load in json\n",
    "    with open(file_path, 'r') as fin:\n",
    "        for l in fin.readlines():\n",
    "            data.append(json.loads(l))\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38687 books in 1 seconds.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing.dummy import Pool as Threadpool\n",
    "from itertools import chain\n",
    "\n",
    "current_path = os.getcwd() + '/'\n",
    "\n",
    "start = timer()\n",
    "\n",
    "# List of files to read in\n",
    "saved_files = [current_path + 'data/' + x for x in os.listdir(current_path + 'data/')]\n",
    "saved_files = [file for file in saved_files if file.endswith('ndjson')]\n",
    "\n",
    "# Create a threadpool for reading in files\n",
    "threadpool = Threadpool(processes = 10)\n",
    "\n",
    "# Read in the files as a list of lists\n",
    "results = threadpool.map(read_data, saved_files)\n",
    "\n",
    "# Flatten the list of lists to a single list\n",
    "book_list = list(chain(*results))\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print(f'Found {len(book_list)} books in {round(end - start)} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already saved.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(current_path + 'found_books_filtered.ndjson'):\n",
    "    with open(current_path + 'found_books_filtered.ndjson', 'w+') as fout:\n",
    "        for book in book_list:\n",
    "             fout.write(json.dumps(book) + '\\n')\n",
    "    print('Books saved.')\n",
    "else:\n",
    "    print('Files already saved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
