{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target\n",
    "\n",
    "Assume we have trained $K$ models and their predicted probabilities/distributions on the training set can be written as a matirx $A^{(k)}$ and we want to fuse them like $\\sum_{k=1}^K \\alpha_k A^{(k)}$ with constraint $\\sum_{k=1}^K \\alpha_k=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "The method is to maximize loglikelihood\n",
    "$$\\frac{1}{N}\\sum_{i=1}^N (\\log\\sum_{k=1}^K \\alpha_k A^{(k)})_{i, :} \\cdot l_{i,:}$$\n",
    "Here $l$ is the label matrix and $l_{i,j}=1$ if the label of $i$-th sample is $j$\n",
    "\n",
    "To be exact, let $A = \\sum_{k=1}^K \\alpha_k A^{(k)}$, .i.e $A_{i, j} = \\sum_{k=1}^K \\alpha_k A^{(k)}_{i, j}$, and there are in total $L$ classes, the average loss is\n",
    "$$\\mathcal{L}=\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^L\\log(A_{i,j})\\cdot l_{i,j}=\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^L\\log(\\sum_{k=1}^K \\alpha_k A^{(k)}_{i,j})\\cdot l_{i,j}$$\n",
    "\n",
    "Now we consider the gradient w.r.t $\\alpha_i, i=1, 2, \\dots, K-1$\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_m} &= \\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^L\\frac{\\partial \\log(\\sum_{k=1}^K \\alpha_k A^{(k)}_{i,j})}{\\partial \\alpha_m}\\cdot l_{i,j}\\\\\n",
    "&=\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^L\\frac{\\frac{\\partial \\sum_{k=1}^K \\alpha_k A^{(k)}_{i,j}}{\\partial \\alpha_m}}{\\sum_{k=1}^K \\alpha_k A^{(k)}_{i,j}}\\cdot l_{i,j}\\\\\n",
    "&=\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^L\\frac{\\frac{\\partial (\\alpha_m A^{(m)}_{i, j}+\\alpha_K A^{(K)}_{i,j})}{\\partial \\alpha_m}}{\\sum_{k=1}^K \\alpha_k A^{(k)}_{i,j}}\\cdot l_{i,j}\\\\\n",
    "&=\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^L\\frac{A^{(m)}_{i, j} - A^{(K)}_{i, j}}{\\sum_{k=1}^K \\alpha_k A^{(k)}_{i,j}}\\cdot l_{i,j}\n",
    "\\end{align*}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Now we consider some regularization : we penalize the vector $\\mathbf{\\alpha}$ if it's too far away from uniform distribution. We consider two kinds of regularizations :\n",
    "\n",
    "### KL distance \n",
    "\n",
    "The new target to maximize would be\n",
    "      \n",
    "$$\\mathcal{L} -\\lambda (-\\sum_{k=1}^K \\frac{1}{K}\\log(K\\alpha_k)) = \\mathcal{L} + \\frac{\\lambda}{K}\\sum_{k=1}^K \\log(K\\alpha_k) $$\n",
    "\n",
    "And the new gradient becomes \n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_m} + \\frac{\\lambda}{K}\\cdot(\\frac{1}{\\alpha_m}-\\frac{1}{\\alpha_K})$$\n",
    "### $L_2$ norm\n",
    "The new target to maximize would be\n",
    "      \n",
    "$$\\mathcal{L} -\\lambda \\sum_{k=1}^K (\\alpha_k-\\frac{1}{K})^2$$\n",
    "\n",
    "And the new gradient becomes \n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_m} -2\\lambda(\\alpha_m-\\frac{1}{K})-2\\lambda(-1)(\\alpha_K-\\frac{1}{K})=\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_m}-2\\lambda(\\alpha_m - \\alpha_K)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from multiprocessing import Pool\n",
    "from transforms import *\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "features = ['Parameter'+str(i) for i in [5, 6, 7, 8, 9, 10]]\n",
    "\n",
    "training = pd.read_csv('first_round_training_data.csv')\n",
    "code = {'Pass':1, 'Good':2, 'Excellent':3, 'Fail':0}\n",
    "training['new_Quality'] = training['Quality_label'].apply(lambda x : code[x])\n",
    "\n",
    "L = np.zeros((training.shape[0], 4))\n",
    "labels = ['Fail', 'Pass', 'Good', 'Excellent']\n",
    "for i in range(4):\n",
    "    L[:, i] = (training['Quality_label'].values==labels[i]).astype('float')\n",
    "\n",
    "N = 10\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N, shuffle=True, random_state=302)\n",
    "indices = []\n",
    "for train_index, test_index in skf.split(training[features], training[['new_Quality']]):\n",
    "    indices.append([train_index, test_index])\n",
    "\n",
    "group_columns = []\n",
    "for group in range(100):\n",
    "    name = 'group_%s'%group\n",
    "    group_columns.append(name)\n",
    "    training[name] = 0\n",
    "    kfold=StratifiedKFold(n_splits=120, shuffle=True, random_state=group)\n",
    "    split=kfold.split(training[features], training['new_Quality'])\n",
    "    i = 0\n",
    "    for train_index, valid_index in split:\n",
    "        training.iloc[valid_index,-1] = i\n",
    "        i+=1\n",
    "\n",
    "pool = Pool(processes = N_split)\n",
    "\n",
    "#names = []\n",
    "#A_list = []\n",
    "#for name in names:\n",
    "#    A_list.append(pd.read_csv('{}.csv'.format(name)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(A_list, L, alpha, lambd, regularization='KL'):\n",
    "    '''\n",
    "    Inputs:\n",
    "    =======\n",
    "    A_list : list of numpy arrays containing prediction probabilities\n",
    "    L : true 0-1 numpy array\n",
    "    alpha : length K vector as fusion weights\n",
    "    lambda : value of lambda, the strength of regularization term\n",
    "    regularization : string for choice of regularization. default 'KL'. Can be 'L2'. If neither, 'KL' would be use\n",
    "    \n",
    "    Return:\n",
    "    =======\n",
    "    grads : lenght K-1 vector, which is the gradient of first K-1 elements of alpha\n",
    "    '''\n",
    "    if not regularization in ['KL', 'L2']:\n",
    "        regularization = 'KL'\n",
    "    \n",
    "    N = A_list[0].shape[0]\n",
    "    K = len(A_list)\n",
    "    A = np.zeros(A_list[0].shape)\n",
    "    for k in range(K):\n",
    "        A += alpha[k] * A_list[k]\n",
    "    grad = np.zeros(K-1)\n",
    "    for i in range(K-1):\n",
    "        grad[i] = np.mean(np.sum((A_list[i] - A_list[-1])/(A+1e-6)*L, axis=1))#partial L/partial alpha_i\n",
    "    for i in range(K-1):\n",
    "        if regularization=='KL':\n",
    "            grad[i] += lambd/K * (1/alpha[i]-1/alpha[-1])\n",
    "        else:\n",
    "            grad[i] -= 2*lambd*(alpha[i] - alpha[-1])\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(A_list, L, step_size, lambd=0, regularization='KL', max_iter=100000, eps=1e-7):\n",
    "    K = len(A_list)\n",
    "    alpha = np.ones(K)/K\n",
    "    counter = 0\n",
    "    log = []\n",
    "    if not regularization in ['KL', 'L2']:\n",
    "        regularization = 'KL'\n",
    "    while counter<max_iter:\n",
    "        log.append(target_function(A_list, alpha, L, lambd, regularization))\n",
    "        grads = np.zeros(K-1)\n",
    "        grads = gradient(A_list, L, alpha, lambd, regularization)\n",
    "        if np.mean(grads**2)/np.mean(alpha[:(K-1)]**2) < eps:\n",
    "            return alpha, log, counter\n",
    "        alpha[:(K-1)] += step_size * grads\n",
    "        alpha[-1] = 1 - np.sum(alpha[:(K-1)])\n",
    "        counter += 1\n",
    "    return alpha, log, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function(A_list, alpha, L, lambd, regularization):\n",
    "    K = len(A_list)\n",
    "    A = np.zeros(A_list[0].shape)\n",
    "    for i in range(K):\n",
    "        A += A_list[i] * alpha[i]\n",
    "    likelihood = np.mean(np.sum(np.log(A)*L, axis=1))\n",
    "    if regularization == 'KL':\n",
    "        reg = lambd/K * np.sum(np.log(K*alpha))\n",
    "    else:\n",
    "        reg = -lambd * np.sum((lambd - 1/K)**2)\n",
    "    return result, result + reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.10229891, -1.0208783 , -2.52738293, -0.01370089],\n",
       "       [-0.03743895, -0.6110007 , -0.48760411, -0.93041988],\n",
       "       [-0.93339405, -0.75483713, -1.12733242, -0.75964912],\n",
       "       [ 2.97831053, -2.38009137,  1.66008827, -0.82310972],\n",
       "       [-0.38318593, -0.07503572, -1.09066392,  0.29695162],\n",
       "       [-0.98612851, -0.48129729, -2.3183331 , -0.53266503],\n",
       "       [ 1.15484121, -1.39424975,  0.32953798, -1.14835492],\n",
       "       [ 0.31390838, -1.56448825,  0.12998619,  0.33470588],\n",
       "       [ 0.29359795,  0.96648361, -0.70890427,  0.73794366],\n",
       "       [ 0.35818055,  1.67546872, -1.14301274, -1.47683964]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.standard_normal((10, 4))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 1, 0, 3, 1, 0, 3, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
